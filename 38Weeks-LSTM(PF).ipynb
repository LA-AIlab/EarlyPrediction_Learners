{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import  Dropout\n",
    "import keras\n",
    "np.random.seed()\n",
    "\n",
    "#from keras.models import load_model\n",
    "from keras.layers import LSTM, Masking\n",
    "import tensorflow as tf\n",
    "import keras.callbacks\n",
    "import sys\n",
    "import os\n",
    "import pandas\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model\n",
    " \n",
    "\n",
    "\n",
    "df1=pd.read_csv('38Weeks-PassFail-Data.csv', low_memory=False)\n",
    "print(df1.shape)\n",
    "\n",
    "df1=df1.sort_values(by=['id1','week_id'])\n",
    "\n",
    "# factorizing the final result \n",
    "d=[ 'final_result']\n",
    "\n",
    "for val in d:\n",
    "    labels,levels = pd.factorize(df1[val])\n",
    "    df1[val] = labels\n",
    "\n",
    "print(df1.head())\n",
    "\n",
    "\n",
    "\n",
    "# computing sequences #\n",
    "############################# Same length window ########################\n",
    "def make_frames1(df):\n",
    "    \n",
    "    \n",
    "    trys = []\n",
    "    big_flat=[]\n",
    "    \n",
    "    for x in range(0,38): \n",
    "        \n",
    "        trys.append(list(df.iloc[0:x+1,1:21].values[x])) \n",
    "        \n",
    "        flat_list=[]\n",
    "        for sublist in trys:         \n",
    "            for item in sublist:\n",
    "                flat_list.append(item)\n",
    "        \n",
    "        \n",
    "        big_flat.append(flat_list) # works fine...big_flat appends every flat_list\n",
    "\n",
    "    \n",
    "    #padding\n",
    "    for i in range(0,38):\n",
    "        op=len(big_flat[i])\n",
    "        for w in range(0,(760-op)):\n",
    "            big_flat[i].append(-1)\n",
    "    \n",
    "    data_a=pd.DataFrame({\"bigflat\":big_flat}) \n",
    "    \n",
    "    return data_a\n",
    "\n",
    "\n",
    "# function for making sequences #\n",
    "def make_labels(df):\n",
    "    labels=[]\n",
    "    for x in range(0,38):\n",
    "        labels.append(list(df.iloc[0:x+1,1:2].values[x]))\n",
    "    return labels\n",
    "\n",
    "#making sequences for labeles #\n",
    "df_label=pd.DataFrame({})\n",
    "for num in df1['id1'].unique():    \n",
    "    t=make_labels(df1[df1['id1']==num])#make frame fr each unique id\n",
    "    #t=t.sort_values(by=['id1'])\n",
    "    \n",
    "    print(\"DF for one id:\",t)\n",
    "    #print(\"t\",t)\n",
    "    df_label=df_label.append(t) # the total df that has all the row for each id\n",
    "                        # 0-37 for 1 id, 38-75 for 2nd id...(75+38=113)....\n",
    "    \n",
    "print(\"DF-labels: \", df_label)\n",
    "\n",
    "print(type(df_label))\n",
    "print(df1.head())\n",
    "\n",
    "\n",
    "df1=df1.drop(['final_result'], axis=1)\n",
    "\n",
    "\n",
    "#fr each unique id, it will create dataframes, \n",
    "# appending 0-37 rows for each unique id \n",
    "dfnew=pd.DataFrame({})\n",
    "for num in df1['id1'].unique():    \n",
    "    t=make_frames1(df1[df1['id1']==num])#make frame fr each unique id       \n",
    "    print(\"DF for one id:\",t)\n",
    "    dfnew=dfnew.append(t) # the total df that has all the row for each id\n",
    "                        # 0-37 for 1 id, 38-75 for 2nd id...(75+38=113)....\n",
    "    \n",
    "print(\"DF-Final: \", dfnew)\n",
    "type(dfnew)\n",
    "\n",
    "df_col = pd.DataFrame(dfnew['bigflat'].values.tolist()) # converting seq to list\n",
    "\n",
    "\n",
    "print(df_col.shape)\n",
    "\n",
    "print(type(df_col))\n",
    "print(df_col.shape[0])\n",
    "print(df_col.shape[1])\n",
    "print(df_col.shape)\n",
    "\n",
    "\n",
    "X_train= df_col.iloc[0:596866,] \n",
    "y_train= df_label.iloc[0:596866,]\n",
    "X_test= df_col.iloc[596866:,]\n",
    "y_test=df_label.iloc[596866:,]\n",
    "\n",
    "\n",
    "print(type(X_train))\n",
    "print(type(y_train))\n",
    "print(type(X_test))\n",
    "print(type(y_test))\n",
    "print(\"xtrain\",X_train.shape)\n",
    "print(\"ytrain\",y_train.shape)\n",
    "print(\"xtest\",X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(X_train.shape[1])\n",
    "\n",
    "\n",
    "X_train=X_train.values.reshape((X_train.shape[0], X_train.shape[1] ))\n",
    "y_train=y_train.values.reshape((y_train.shape[0],1 ))\n",
    "X_test=X_test.values.reshape((X_test.shape[0],X_test.shape[1]))\n",
    "y_test=y_test.values.reshape((y_test.shape[0],1))\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(\"type xtrain\", type(X_train))\n",
    "print(y_train.shape)\n",
    "print(\"type ytrain\", type(y_train))\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(X_train.shape[0])\n",
    "print(X_train.shape[1])\n",
    "print(X_test.shape[0])\n",
    "\n",
    "\n",
    "X_train=X_train.reshape((X_train.shape[0],38,20))\n",
    "X_test=X_test.reshape((X_test.shape[0],38,20))\n",
    "\n",
    "\n",
    "# In[115]:\n",
    "\n",
    "\n",
    "print(X_train.shape[0])\n",
    "print(X_train.shape[1])\n",
    "print(\"xtrain.shape[2]\",X_train.shape[2])\n",
    "print(X_train.shape)\n",
    "print(X_test.shape[0])\n",
    "print(X_test.shape[1])\n",
    "print(X_test.shape[2])\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape[1])\n",
    "\n",
    "\n",
    "# In[116]:\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "\n",
    "\n",
    "# record history of training\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.accuracies.append(logs.get('acc'))\n",
    "\n",
    "\n",
    "total = len(sys.argv)\n",
    "cmdargs = str(sys.argv)\n",
    "\n",
    "print (\"Script name: %s\" % str(sys.argv[0]))\n",
    "checkpoint = None\n",
    "if len(sys.argv) == 2:\n",
    "    if os.path.exists(str(sys.argv[1])):\n",
    "        print (\"Checkpoint : %s\" % str(sys.argv[1]))\n",
    "        checkpoint = str(sys.argv[1])\n",
    "        print(\"check point\")\n",
    "\n",
    "\n",
    "#LSTM model\n",
    "sequence_length=38\n",
    "nb_features = X_train.shape[2] #20\n",
    "nb_out = y_train.shape[1] #1\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Masking(mask_value=-1, input_shape=(sequence_length, nb_features)))\n",
    "\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=300,\n",
    "         return_sequences=True))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(LSTM(\n",
    "          units=200,\n",
    "          return_sequences=True))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(LSTM(\n",
    "          units=100,\n",
    "          return_sequences=False))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "# model.add(LSTM(\n",
    "#           units=50,\n",
    "#           return_sequences=False))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(units=nb_out, activation='tanh'))\n",
    "\n",
    "if checkpoint:\n",
    "    model.load_weights(checkpoint)\n",
    "\n",
    "file_name = os.path.basename(sys.argv[0]).split('.')[0]\n",
    "check_cb = keras.callbacks.ModelCheckpoint('LSTM-PassFail/'+ file_name + '.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                           monitor='val_loss',\n",
    "                                           verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "history = LossHistory()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',precision,recall, f1])\n",
    "\n",
    "pandas.DataFrame(model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=4560,\n",
    "          epochs=60, shuffle=True, callbacks=[ check_cb, history]).history).to_csv(\"38week-PF-adam.csv\")\n",
    "\n",
    "\n",
    "model.save('38-weeksModel-PassFail.h5')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
